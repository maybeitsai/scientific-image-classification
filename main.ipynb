{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77514cf2",
   "metadata": {},
   "source": [
    "# **Proyek Klasifikasi Gambar CNN: Scientific Images**\n",
    "\n",
    "## **Tujuan**\n",
    "Membangun, melatih, dan mengevaluasi model Convolutional Neural Network (CNN) untuk mengklasifikasikan gambar ilmiah ke dalam 6 kategori (Blot-Gel, FACS, Histopathology, Macroscopy, Microscopy, Non-scientific) dengan akurasi minimal 95% pada data training dan testing, serta memenuhi semua kriteria yang ditetapkan.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fee82c",
   "metadata": {},
   "source": [
    "## **Langkah 1: Pengaturan Lingkungan dan Unduh Dataset**\n",
    "Pertama, kita perlu menyiapkan lingkungan kerja dan mengunduh dataset dari Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5fad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instal library yang mungkin belum ada (jalankan di terminal atau cell notebook)\n",
    "# !pip install tensorflow numpy matplotlib split-folders kaggle\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import splitfolders # Library untuk membagi dataset\n",
    "import pathlib\n",
    "import PIL # Python Imaging Library untuk cek gambar\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi Kaggle (jika belum)\n",
    "# Anda perlu mengunggah file kaggle.json Anda\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22fe956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/rushilprajapati/data-final\n",
      "License(s): apache-2.0\n"
     ]
    }
   ],
   "source": [
    "# Unduh dataset dari Kaggle\n",
    "# !kaggle datasets download -d rushilprajapati/data-final -p ./data --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentukan path utama dataset setelah di-unzip\n",
    "# Sesuaikan path ini jika Anda meletakkannya di lokasi berbeda\n",
    "dataset_base_dir = '/data' # Ganti jika perlu\n",
    "dataset_base_path = pathlib.Path(dataset_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245d955",
   "metadata": {},
   "source": [
    "### **Penjelasan Langkah 1:**\n",
    "1.  **Instalasi & Impor:** Memasang dan mengimpor library esensial seperti TensorFlow, NumPy, Matplotlib, dan `splitfolders`.\n",
    "2.  **Kaggle API:** Menyiapkan Kaggle API untuk mengunduh dataset (komentar `#` bisa dihilangkan jika dijalankan di lingkungan seperti Colab).\n",
    "3.  **Unduh & Unzip:** Mengunduh dataset dari URL Kaggle yang diberikan dan mengekstraknya.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed28df0b",
   "metadata": {},
   "source": [
    "## **Langkah 2: Eksplorasi Data Analisis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Eksplorasi Awal: Cek Kelas dan Tampilkan Sampel ---\n",
    "print(\"\\n--- Eksplorasi Data Awal ---\")\n",
    "class_names = sorted([item.name for item in dataset_base_path.glob('*') if item.is_dir()])\n",
    "num_classes = len(class_names)\n",
    "print(f\"Kelas yang ditemukan ({num_classes}): {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan 2 sampel per kelas\n",
    "plt.figure(figsize=(12, 8))\n",
    "print(\"Menampilkan 2 sampel gambar per kelas:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_dir = dataset_base_path / class_name\n",
    "    # Ambil 2 file gambar (jpg atau png)\n",
    "    sample_files = list(class_dir.glob('*.jpg'))[:2] + list(class_dir.glob('*.png'))[:2]\n",
    "    sample_files = sample_files[:2] # Pastikan hanya 2\n",
    "\n",
    "    if not sample_files:\n",
    "        print(f\"  Peringatan: Tidak ada sampel ditemukan untuk kelas '{class_name}'\")\n",
    "        continue\n",
    "\n",
    "    for j, file_path in enumerate(sample_files):\n",
    "        plt.subplot(num_classes, 2, i * 2 + j + 1)\n",
    "        try:\n",
    "            img = PIL.Image.open(file_path)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Kelas: {class_name}\", fontsize=10)\n",
    "            plt.axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Error membuka {file_path}: {e}\")\n",
    "            plt.title(f\"Error: {class_name}\", fontsize=10)\n",
    "            plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2bc7c",
   "metadata": {},
   "source": [
    "## **Langkah 3: Pembagian Dataset (Train, Validation, Test Set)**\n",
    "Kita akan membagi dataset menjadi tiga bagian: training (misal 70%), validation (misal 15%), dan testing (misal 15%) menggunakan library `splitfolders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentukan direktori output untuk dataset yang sudah dibagi\n",
    "output_dir = 'data/scientific_images_split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dda5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus direktori output jika sudah ada untuk memastikan pembagian bersih\n",
    "if os.path.exists(output_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(output_dir)\n",
    "    print(f\"Direktori '{output_dir}' yang sudah ada dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53091eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membagi dataset (70% train, 15% val, 15% test)\n",
    "# splitfolders akan membuat struktur folder train/val/test secara otomatis\n",
    "print(f\"Membagi dataset dari '{dataset_base_dir}' ke '{output_dir}'...\")\n",
    "splitfolders.ratio(\n",
    "    dataset_base_dir,\n",
    "    output=output_dir,\n",
    "    seed=42, # Untuk reproduktifitas\n",
    "    ratio=(.7, .15, .15), # Rasio train/val/test\n",
    "    group_prefix=None, # Tidak ada prefix grup file\n",
    "    move=False # Salin file, jangan pindahkan (lebih aman)\n",
    ")\n",
    "\n",
    "print(\"Pembagian dataset selesai.\")\n",
    "print(f\"Dataset terbagi tersedia di: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisikan path untuk setiap set\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "val_dir = os.path.join(output_dir, 'val')\n",
    "test_dir = os.path.join(output_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifikasi jumlah gambar di setiap set\n",
    "def count_files(directory):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        count += len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    return count\n",
    "\n",
    "print(f\"Jumlah gambar training: {count_files(train_dir)}\")\n",
    "print(f\"Jumlah gambar validation: {count_files(val_dir)}\")\n",
    "print(f\"Jumlah gambar testing: {count_files(test_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17f42f",
   "metadata": {},
   "source": [
    "### **Penjelasan Langkah 3:**\n",
    "1.  **Output Directory:** Menentukan lokasi penyimpanan dataset yang sudah dibagi.\n",
    "2.  **Splitfolders:** Menggunakan library `splitfolders` yang praktis untuk membagi dataset berdasarkan rasio yang ditentukan sambil mempertahankan struktur kelas di dalam folder `train`, `val`, dan `test`.\n",
    "3.  **Verifikasi:** Menghitung jumlah file di setiap set untuk memastikan pembagian berhasil.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab0d8f",
   "metadata": {},
   "source": [
    "## **Langkah 4: Preprocessing dan Augmentasi Data**\n",
    "\n",
    "Kita akan menggunakan `ImageDataGenerator` (atau `tf.keras.utils.image_dataset_from_directory` sebagai alternatif modern) untuk:\n",
    "1.  Membaca gambar dari direktori.\n",
    "2.  Mengubah ukuran semua gambar ke ukuran yang seragam untuk menangani ukuran gambar yang beragam.\n",
    "3.  Normalisasi nilai piksel (biasanya ke rentang [0, 1]).\n",
    "4.  Menerapkan augmentasi data pada set pelatihan untuk meningkatkan robustisitas model dan mengurangi overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902fba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat dataset train, validation, test menggunakan tf.data\n",
    "print(\"\\nMembuat pipeline data tf.data...\")\n",
    "\n",
    "# Tentukan parameter\n",
    "IMG_HEIGHT = 224 # Tinggi gambar\n",
    "IMG_WIDTH = 224 # Lebar gambar\n",
    "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "BATCH_SIZE = 128 # Sesuaikan berdasarkan memori GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data dari direktori ke tf.data.Dataset\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    label_mode='categorical' # Penting untuk CategoricalCrossentropy\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    shuffle=False, # Tidak perlu shuffle validation/test\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    label_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pastikan nama kelas konsisten\n",
    "class_names_tfdata = train_dataset.class_names\n",
    "print(f\"Kelas yang dideteksi oleh tf.data: {class_names_tfdata}\")\n",
    "if class_names != class_names_tfdata:\n",
    "    print(\"Peringatan: Urutan kelas berbeda antara eksplorasi awal dan tf.data!\")\n",
    "    # Sebaiknya gunakan class_names_tfdata sebagai referensi utama\n",
    "    class_names = class_names_tfdata\n",
    "    num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57281be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Augmentasi Data sebagai Layer Keras ---\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomContrast(0.2),\n",
    "        layers.RandomBrightness(0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0449cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimasi Pipeline Data ---\n",
    "# Normalisasi (dipisah agar bisa diterapkan setelah augmentasi jika augmentasi di luar model)\n",
    "# atau bisa langsung di model pre-trained jika menggunakan preprocess_input\n",
    "# Di sini kita normalisasi manual [0, 1]\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "# Terapkan augmentasi dan normalisasi pada training set\n",
    "# Terapkan normalisasi saja pada validation dan test set\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "validation_dataset = validation_dataset.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"Pipeline tf.data dengan augmentasi (train) dan prefetching siap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07f427",
   "metadata": {},
   "source": [
    "### **Penjelasan Langkah 4:**\n",
    "1.  **Parameter:** Menentukan ukuran gambar input (`IMG_WIDTH`, `IMG_HEIGHT`) dan ukuran batch (`BATCH_SIZE`). Ukuran gambar yang seragam untuk menangani masalah ukuran gambar yang berbeda-beda.\n",
    "2.  **ImageDataGenerator:** Membuat dua instance: satu untuk training dengan augmentasi (rotasi, geser, zoom, flip) dan satu lagi untuk validasi/testing hanya dengan normalisasi.\n",
    "3.  **Data Generators:** Menggunakan metode `flow_from_directory` untuk membuat generator yang akan membaca gambar dari folder, melakukan preprocessing/augmentasi, dan menyajikannya dalam batch ke model. `class_mode='categorical'` digunakan karena ini adalah masalah klasifikasi multi-kelas.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5fee7",
   "metadata": {},
   "source": [
    "## **Langkah 5: Membangun Model CNN (Sequential, Conv2D, Pooling)**\n",
    "\n",
    "Kita akan membangun model CNN menggunakan Keras Sequential API, yang terdiri dari lapisan `Conv2D` dan `MaxPooling2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9322ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membangun Model Sequential\n",
    "model = models.Sequential([\n",
    "    # Lapisan Konvolusi Pertama\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Lapisan Konvolusi Kedua\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', ),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Lapisan Konvolusi Ketiga\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Lapisan Konvolusi Keempat (Opsional, bisa ditambah/dikurangi sesuai kebutuhan)\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Meratakan output untuk lapisan Dense\n",
    "    layers.Flatten(),\n",
    "\n",
    "    # Lapisan Dense (Fully Connected)\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5), # Dropout untuk regularisasi\n",
    "\n",
    "    # Lapisan Output\n",
    "    layers.Dense(num_classes, activation='softmax') # Softmax untuk multi-kelas\n",
    "])\n",
    "\n",
    "# Menampilkan ringkasan model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1a81a",
   "metadata": {},
   "source": [
    "### **Penjelasan Langkah 5:**\n",
    "1.  **Sequential API:** Model dibangun lapis demi lapis.\n",
    "2.  **Conv2D:** Lapisan konvolusi mengekstraksi fitur dari gambar menggunakan filter (kernel). Fungsi aktivasi 'relu' umum digunakan. `input_shape` ditentukan hanya di lapisan pertama.\n",
    "3.  **MaxPooling2D:** Lapisan pooling mengurangi dimensi spasial (tinggi dan lebar) representasi fitur, membuatnya lebih robust terhadap variasi posisi fitur dan mengurangi komputasi.\n",
    "4.  **Flatten:** Mengubah output dari lapisan konvolusi/pooling (yang berupa tensor 3D) menjadi vektor 1D agar bisa dimasukkan ke lapisan Dense.\n",
    "5.  **Dense:** Lapisan fully connected untuk melakukan klasifikasi berdasarkan fitur yang diekstraksi.\n",
    "6.  **Dropout:** Teknik regularisasi untuk mencegah overfitting dengan menonaktifkan sebagian neuron secara acak selama training.\n",
    "7.  **Output Layer:** Lapisan Dense terakhir dengan jumlah neuron sama dengan jumlah kelas dan fungsi aktivasi `softmax` untuk menghasilkan probabilitas untuk setiap kelas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eadf174",
   "metadata": {},
   "source": [
    "### **Langkah 6: Kompilasi Model**\n",
    "\n",
    "Mengkonfigurasi model untuk training dengan menentukan optimizer, loss function, dan metrik evaluasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c192e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompilasi model\n",
    "model.compile(\n",
    "    optimizer='adam', # Optimizer yang umum digunakan\n",
    "    loss='categorical_crossentropy', # Loss function untuk multi-kelas kategorikal\n",
    "    metrics=['accuracy'] # Metrik untuk evaluasi\n",
    ")\n",
    "\n",
    "print(\"Model berhasil dikompilasi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72beb692",
   "metadata": {},
   "source": [
    "**Penjelasan Langkah 6:**\n",
    "1.  **Optimizer:** Algoritma yang digunakan untuk memperbarui bobot model berdasarkan loss (misalnya, 'adam', 'rmsprop', 'sgd').\n",
    "2.  **Loss Function:** Mengukur seberapa baik performa model pada data training. `categorical_crossentropy` cocok untuk klasifikasi multi-kelas dengan label one-hot encoded (yang secara otomatis dihasilkan oleh `ImageDataGenerator` dengan `class_mode='categorical'`).\n",
    "3.  **Metrics:** Metrik yang digunakan untuk mengevaluasi performa model selama training dan testing. 'accuracy' adalah pilihan umum untuk klasifikasi.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239b898",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
